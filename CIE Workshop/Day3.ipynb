{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Problem of Overfitting\n",
    "- Overfitting: If we have too many features, the learned hypothesis may fit the training set very well (cost function will be very low), but fail to generalize to new examples (predict prices on new examples).\n",
    "- Addressing overfitting:\n",
    "    - Increase number of training examples\n",
    "    - Reduce number of features\n",
    "        - Manually select which features to keep\n",
    "        - Model selection algorithm\n",
    "    - Regularization\n",
    "        - Eliminating features results in loss of information\n",
    "        - Keep all features, but reduce magnitude/values of parameters $\\theta_j$\n",
    "        - Works well when we have a lot of features, each of which contributes a bit to predicting $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Addressing Overfitting\n",
    "- Collect More Data\n",
    "    - Fixes high variance\n",
    "- Select Features\n",
    "    - Features selection algorithm\n",
    "- Reduce size of Parameters\n",
    "    - Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Cost Function with Regularization:\n",
    "$$J(\\theta) = \\frac{1}{2m} \\left[ \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2 + \\lambda \\sum_{j=1}^n \\theta_j^2 \\right]$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Regularized Linear Regression:\n",
    "- Gradient Descent:\n",
    "- Repeat {\n",
    "    $$\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_0^{(i)}$$\n",
    "- Derivation Steps:\n",
    "- $$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{\\partial}{\\partial \\theta_j} \\left[ \\frac{1}{2m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 \\right]$$\n",
    "- $$= \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_j^{(i)} + \\frac{\\lambda}{m} \\theta_j$$\n",
    "- $$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)$$\n",
    "- $$\\theta_j := \\theta_j - \\alpha \\left[ \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_j^{(i)} + \\frac{\\lambda}{m} \\theta_j \\right]$$\n",
    "- $$\\theta_j := \\theta_j \\left( 1 - \\alpha \\frac{\\lambda}{m} \\right) - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Advanced Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Neural Networks\n",
    "- algorithms to mimic the brain\n",
    "- applications: speech recognition, computer vision, robotics, etc.\n",
    "- model computation as a network of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Hidden Layers:\n",
    "- grid search for number of hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Activation Value:\n",
    "- $a_i^{(j)}$ = \"activation\" of unit $i$ in layer $j$\n",
    "- $a_i^{(j)}$ is given by $a_i^{(j)} = g(z_i^{(j)})$ = g($\\theta_{i0}^{(j)} {a_0^{(j-1)}} + \\theta_{i1}^{(j)} {a_1^{(j-1)}} + \\dots + \\theta_{in}^{(j)} {a_n^{(j-1)}}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Forward Propagation:\n",
    "- $a_1^{(2)} = g(\\theta_{10}^{(1)} x_0 + \\theta_{11}^{(1)} x_1 + \\theta_{12}^{(1)} x_2 + \\theta_{13}^{(1)} x_3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Representation:\n",
    "```python\n",
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "layer_1 = Dense(units=2, activation='sigmoid')\n",
    "a1 = layer_1(x)\n",
    "layer_2 = Dense(units=1, activation='sigmoid')\n",
    "a2 = layer_2(a1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Building Neural Networks\n",
    "- code:\n",
    "```python\n",
    "layer_1 = Dense(units=2, activation='sigmoid')\n",
    "layer_2 = Dense(units=1, activation='sigmoid')\n",
    "model = Sequential([layer_1, layer_2])\n",
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.1))\n",
    "model.fit(x, y, epochs=1000, verbose=False)\n",
    "print(model.predict(x))\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train a Neural Netwrok in TensorFlow\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "#defining the model\n",
    "# here the model has 3 layers\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(units=25, activation=\"sigmoid\"),\n",
    "        Dense(units=15, activation=\"sigmoid\"),\n",
    "        Dense(units=1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "model.compile(loss=BinaryCrossentropy()) # for logistic regression\n",
    "model.compile(loss=MeanSquaredError()) # for linear regression\n",
    "\n",
    "model.fit(x, y, epochs=1000) # does gradient descent\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions:\n",
    "- Sigmoid: $g(z) = \\frac{1}{1 + e^{-z}}$ -> Binary Classification\n",
    "- ReLU: $g(z) = \\max(0, z)$ -> Hidden Layers\n",
    "- Linear: $g(z) = z$ -> Regression\n",
    "- Choosing Activation Function:\n",
    "    - Sigmoid: $0 \\leq h_\\theta(x) \\leq 1$\n",
    "    - ReLU: $h_\\theta(x)$ can be much greater than 1\n",
    "    - Linear: $h_\\theta(x)$ can be much less than -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Activation Function:\n",
    "- Output Layer:\n",
    "    - Linear: $h_\\theta(x) = \\theta^T x$ for regression, y= +ve or -ve\n",
    "    - sigmoid: $h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}}$ for binary classification, y = 0 or 1\n",
    "    - relu: $h_\\theta(x) = \\max(0, \\theta^T x)$ for multi-class classification, y = 0 or +ve\n",
    "\n",
    "- Hidden Layers:\n",
    "    - relu: $g(z) = \\max(0, z)$ is faster to compute than sigmoid, it is more commonly used"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Classification:\n",
    "- activation function for output layer: softmax\n",
    "- softmax: $h_\\theta(x)_i = \\frac{e^{\\theta_i^T x}}{\\sum_{j=1}^k e^{\\theta_j^T x}}$\n",
    "- softmax depends on all other units in the output layer\n",
    "- logistic regression is a special case of softmax regression as k = 2, we can get the same result using sigmoid function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost:\n",
    "- logistic regression: $J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)} \\log h_\\theta(x^{(i)}) + (1 - y^{(i)}) \\log (1 - h_\\theta(x^{(i)})) \\right]$, $h_\\theta(x^{(i)}) = \\frac{1}{1 + e^{-\\theta^T x^{(i)}}}$\n",
    "    - here replacing 1 - y with y' and 1 - h with h' we get: $J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)} \\log h_\\theta(x^{(i)}) + y'^{(i)} \\log h'_\\theta(x^{(i)}) \\right]$\n",
    "- similarly for softmax regression: $J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^k \\left[ y_j^{(i)} \\log h_\\theta(x^{(i)})_j \\right]$\n",
    "- for softmax we use loss function called sparse_categorical_crossentropy\n",
    "```python \n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "model.compile(loss=SparseCategoricalCrossentropy())\n",
    "```    \n",
    "### Numerical Roundoff Error:\n",
    "- to avoid specify from_logits=True which will apply softmax activation function without rounding off\n",
    "```python\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "model.compile(loss=SparseCategoricalCrossentropy(from_logits=True))\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Label Classification:\n",
    "- multi-label classification: $y \\in \\{0, 1\\}^4$\n",
    "- multi-class classification: $y \\in \\{0, 1, 2, 3\\}$\n",
    "- multi class is different from multi-label as multi-class is mutually exclusive"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Algorithm:\n",
    "- adaptive moment estimation\n",
    "- differs from gradient descent in the way it calculates the learning rate\n",
    "- alpha value is calculated for each parameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Procedure:\n",
    "- train test split: 80% train, 20% test\n",
    "- it is important to shuffle the data before splitting\n",
    "- it is done to avoid bias in the data and to avoid overfitting\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection:\n",
    "- train test split: 60% train, 20% validation, 20% test\n",
    "- Cross validation is used when the data is small, it helps to get a better estimate of the model performance\n",
    "- k-fold cross validation: 60% train, 20% validation, 20% test, in this case we have 5 folds, here we train the model 5 times and take the average of the validation accuracy, then the model is tested on the test data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance:\n",
    "- bias: error due to wrong assumptions\n",
    "- variance: error due to sensitivity to small fluctuations in the training set\n",
    "- bias is error with respect to the training set\n",
    "- variance is error with respect to the test set\n",
    "- For Underfitting:\n",
    "    - high bias\n",
    "    - low variance\n",
    "    - J_train is high\n",
    "    - J_cv is high\n",
    "- For Overfitting:\n",
    "    - low bias\n",
    "    - high variance\n",
    "    - J_train is low\n",
    "    - J_cv is high\n",
    "- For Good Fit:\n",
    "    - low bias\n",
    "    - low variance        \n",
    "    - J_train is low\n",
    "    - J_cv is low"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance for Regularization:\n",
    "- Large $\\lambda$:\n",
    "    - high bias\n",
    "    - low variance\n",
    "    - J_train is high\n",
    "    - J_cv is high\n",
    "- Small $\\lambda$:\n",
    "    - low bias\n",
    "    - high variance\n",
    "    - J_train is low\n",
    "    - J_cv is high        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves:\n",
    "- high bias: $J_{train}(\\theta)$ is high and $J_{cv}(\\theta)$ is high\n",
    "- high variance: $J_{train}(\\theta)$ is low and $J_{cv}(\\theta)$ is high\n",
    "- if there is high bias then increasing the number of training examples will not help\n",
    "- if there is high variance then increasing the number of training examples will help"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging a Learning Algorithm:\n",
    "- Get more training examples -> fixes high variance\n",
    "- Try smaller sets of features -> fixes high variance\n",
    "- Try getting additional features -> fixes high bias\n",
    "- Try adding polynomial features -> fixes high bias\n",
    "- Try decreasing $\\lambda$ -> fixes high bias\n",
    "- Try increasing $\\lambda$ -> fixes high variance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network and bias/variance:\n",
    "- Does it do well on training set?\n",
    "    - No: high bias, try bigger network, train longer, different NN architecture\n",
    "    - Yes: Does it do well on Cross Validation set?\n",
    "        - No: high variance, try more data, regularization, different NN architecture\n",
    "        - Yes: Done\n",
    "- But higher data requires more computation power, so we can use regularization to reduce variance\n",
    "- Regularised MNIST model:\n",
    "```python\n",
    "# l2 is the lambda value\n",
    "layer_1 = tf.keras.layers.Dense(units=25, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))\n",
    "layer_2 = tf.keras.layers.Dense(units=10, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))\n",
    "layer_3 = tf.keras.layers.Dense(units=1, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.001))        \n",
    "model = tf.keras.Sequential([layer_1, layer_2, layer_3])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
