{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision/Recall:\n",
    "- True Positive (TP): correctly predicted positive\n",
    "- True Negative (TN): correctly predicted negative\n",
    "- False Positive (FP): incorrectly predicted positive\n",
    "- False Negative (FN): incorrectly predicted negative\n",
    "- Accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
    "- Precision: TP / (TP + FP), it is the ratio of correctly predicted positive to all predicted positive\n",
    "- Recall: TP / (TP + FN), it is the ratio of correctly predicted positive to all actual positive\n",
    "## Trading off Precision and Recall\n",
    "- Precision and Recall are inversely related, as precision increases, recall decreases\n",
    "- F1 score: 2 * (precision * recall) / (precision + recall)\n",
    "- F1 score is the harmonic mean of precision and recall rather than the arithmetic mean as it punishes extreme values -> if either precision or recall is 0, F1 score is 0 and if both precision and recall are 1, F1 score is 1\n",
    "- The harmonic mean is also used in the F1 score because it is more appropriate for measuring the performance of models that are trying to achieve a balance between precision and recall. For example, a model that is being used to classify spam emails would ideally have both high precision and high recall. However, it is often necessary to sacrifice some precision in order to improve recall, or vice versa. The harmonic mean allows us to measure the overall performance of the model, even if it is not able to achieve perfect precision and recall.\n",
    "\n",
    "In conclusion, the harmonic mean is used in the F1 score because it is more sensitive to low values and it is more appropriate for measuring the performance of models that are trying to achieve a balance between precision and recall."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees:\n",
    "- Decision trees are a type of supervised learning algorithm that is mostly used for classification problems. Surprisingly, even after being fairly simple and easy to understand, decision trees are powerful. Often decision trees are at the top of any machine learning algorithm. Decision trees are also the building blocks of some of the more advanced algorithms like random forests, gradient boosting etc.\n",
    "\n",
    "## Decision Tree Learning:\n",
    "- Maximize Purity, purity is the measure of how mixed the labels at a node are\n",
    "- Entropy: measure of impurity in a bunch of examples\n",
    "- p1 = probability of positive examples\n",
    "- p0 = probability of negative examples\n",
    "- p1 + p0 = 1\n",
    "- Entropy: H(p1, p0) = -p1log(p1) - p0log(p0) (base 2 as we are using binary classification)\n",
    "(We use base 2 when calculating entropy in decision trees because entropy is measured in bits. A bit is the smallest unit of information, and it can be either a 0 or a 1. The base 2 logarithm is used because it is the most natural way to measure the amount of information in a dataset.\n",
    "\n",
    "Entropy is a measure of the disorder or uncertainty in a dataset. A dataset with high entropy is very disordered, meaning that there is no clear pattern to the data. A dataset with low entropy is very ordered, meaning that there is a clear pattern to the data.\n",
    "\n",
    "Decision trees use entropy to decide which features to split on. A feature with high entropy is a good candidate for splitting because it will help to reduce the entropy of the dataset. This is because splitting the dataset on a high-entropy feature will create two new datasets, each with lower entropy than the original dataset.)\n",
    "- H(p1) = -p1log(p1) - (1 - p1)log(1 - p1)\n",
    "\n",
    "## Choosing a split:\n",
    "- Split on the feature that has maximum entropy\n",
    "- Information Gain: the amount of entropy reduced from the parent to the child node\n",
    "- Information Gain = H(parent) - [weighted average]H(children)\n",
    "\n",
    "## Decision Tree Algorithm:\n",
    "- Recursively split the data based on the feature that gives the maximum information gain\n",
    "- Stop when we reach a pure state or when we reach a predefined depth\n",
    "Order:\n",
    "- Start with the root node\n",
    "- Find the feature that gives the maximum information gain\n",
    "- Split the data based on the feature\n",
    "- Repeat the process on each child node\n",
    "- Stop when we reach a pure state or when we reach a predefined depth\n",
    "\n",
    "## Regression Trees:\n",
    "- Regression trees are used for predicting continuous values\n",
    "- While calculating information gain, we use variance instead of entropy\n",
    "- Variance: measure of how far a set of numbers is spread out from their average value\n",
    "- Variance = sum((x - mean)^2) / n\n",
    "- Information Gain = Variance(parent) - [weighted average]Variance(children)\n",
    "\n",
    "### Trees are highly sensitive to small changes in the data\n",
    "- Small changes in the data can cause a large change in the final estimated tree\n",
    "- This makes the tree unstable\n",
    "- This problem is solved by using ensemble methods like random forests and gradient boosting\n",
    "- Tree ensemble methods are also used for feature selection\n",
    "- In tree ensemble methods, we build multiple trees and average them to get more stable predictions\n",
    "\n",
    "# Random Forests:\n",
    "- In random forests algorithm we use many trees, and each tree is trained on a different subset of the data\n",
    "- here sampling is done with replacement\n",
    "- Each tree is trained on a random subset of features\n",
    "- Random forests are used for classification and regression tasks\n",
    "\n",
    "# XGBoost:\n",
    "- XGBoost stands for eXtreme Gradient Boosting\n",
    "- Boosted trees intuition:\n",
    "- Boosting is an ensemble technique where new models are added to correct the errors made by existing models\n",
    "\n",
    "```python\n",
    "# classification\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# regression\n",
    "from xgboost import XGBRegressor\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering:\n",
    "- Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups\n",
    "- Clustering is an unsupervised learning technique\n",
    "- Clustering algorithms are used for exploratory data analysis to find hidden patterns or grouping in data\n",
    "## K-Means Clustering:\n",
    "- step1: choose the number of clusters k\n",
    "- step2: select k random points from the data as centroids\n",
    "- step3: assign all the points to the closest cluster centroid\n",
    "- step4: recompute the centroids of newly formed clusters\n",
    "- step5: repeat steps 3 and 4\n",
    "- stop when the centroids are not moving\n",
    "- ```Repeat {\n",
    "    #u is the vector of cluster centroids\n",
    "    #c(i) is the index of cluster centroid closest to x(i)\n",
    "    #assign points to cluster\n",
    "    for i=1 to m:\n",
    "        c(i) := index (from 1 to K) of cluster centroid closest to x(i)\n",
    "                centroid closest to x(i) is argmin ||x(i) - u(j)||^2\n",
    "\n",
    "    #move cluster centroids\n",
    "    for j=1 to K:\n",
    "        u(j) := average (mean) of points assigned to cluster j\n",
    "            u(j) := mean of x(i) s.t. c(i) = j\n",
    "} until convergence\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means optimization objective:\n",
    "- the optimization objective of k-means is to minimize the sum of squared distances between the points and their respective cluster centroid\n",
    "- J(c(1), ..., c(m), u(1), ..., u(k)) = 1/m * sum(||x(i) - u(c(i))||^2)\n",
    "- c(i) = index of cluster centroid closest to x(i)\n",
    "- u(j) = cluster centroid j\n",
    "- k = number of clusters\n",
    "- m = number of training examples\n",
    "\n",
    "### Random Initialization:\n",
    "- Randomly pick k training examples\n",
    "- set u(1), ..., u(k) equal to these k examples\n",
    "- Run k-means\n",
    "- Random initialization can sometimes lead to bad clusters\n",
    "- To solve this problem, we run k-means multiple times from multiple random initializations\n",
    "- We then pick the clustering that gave us the lowest cost\n",
    "- cost function is the sum of squared distances between the points and their respective cluster centroid given by the formula: J(c(1), ..., c(m), u(1), ..., u(k)) = 1/m * sum(||x(i) - u(c(i))||^2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- numpy.linalg.norm(x, ord=None, axis=None, keepdims=False)\n",
    "- this calculates the norm of a vector or matrix given by the formula: ||x|| = sqrt(sum(x(i)^2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection:\n",
    "- Anomaly detection is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data\n",
    "- Anomaly detection is applicable in a variety of domains, such as intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, and detecting ecosystem disturbances\n",
    "## Density Estimation:\n",
    "- Density estimation is the construction of an estimate, based on observed data, of an unobservable underlying probability density function\n",
    "## Gaussian Distribution:\n",
    "- The Gaussian distribution is a continuous function which approximates the exact binomial distribution of events\n",
    "- Properties of Gaussian distribution:\n",
    "    - mean = 0\n",
    "    - variance = 1\n",
    "- probability: $p(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$    \n",
    "# Parameter Estimation:\n",
    "- done by maximum likelihood estimation\n",
    "- Maximum likelihood estimation: given some data, what is the most likely probability distribution that produced the data?, here we are trying to find the parameters of the probability distribution\n",
    "- given a dataset x(1), ..., x(m)\n",
    "- assume that x(i) ~ p(x; \\mu, \\sigma^2)\n",
    "- parameters are $\\mu$ and $\\sigma^2$\n",
    "- likelihood function: $L(\\mu, \\sigma^2) = p(x; \\mu, \\sigma^2) = \\prod_{i=1}^m p(x(i); \\mu, \\sigma^2)$\n",
    "then we find the parameters that maximize the likelihood function"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
