{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Common Activation Functions:\n","- Sigmoid Function\n","formula: $f(x) = \\frac{1}{1+e^{-x}}$\n","```python\n","tf.math.sigmoid(x)\n","```\n","- Tanh Function\n","formula: $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n","```python\n","tf.math.tanh(x)\n","```\n","- ReLU Function\n","formula: $f(x) = max(0,x)$\n","```python\n","tf.nn.relu(x)\n","```"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["class MyDenseLayer(tf.keras.layers.Layer):\n","    def __init__(self, input_dim, output_dim):\n","        super(MyDenseLayer, self).__init__()\n","        # Initialize weights and biases\n","        self.W = self.add_weight([input_dim, output_dim])\n","        self.b = self.add_weight([1, output_dim])\n","    \n","    def call(self, inputs):\n","        # Forward propagate the inputs\n","        z = tf.matmul(inputs, self.W) + self.b\n","        \n","        # Feed through a non-linear activation\n","        output = tf.math.sigmoid(z)\n","        \n","        return output    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["layer = tf.keras.layers.Dense(units=2, activation='sigmoid')"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# stacking \n","n = 4\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(n),\n","    tf.keras.layers.Dense(2)\n","])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# mean squared error loss\n","```python\n","loss = tf.reduce_mean(tf.square(tf.subtract(y, predicted)))\n","loss = tf.keras.losses.MSE(y, predicted)\n","```\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Gradient Descent:\n","```python\n","weights = tf.Variable([tf.random.normal()]) # random initialization\n","\n","while True: \n","    with tf.GradientTape() as g:\n","        loss = compute_loss(weights) # compute loss\n","        gradient = g.gradient(loss, weights) # compute gradient\n","    weights = weights - lr * gradient # update weights\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[" # Backpropagation:\n"," "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["using chain rule to compute the gradient of the loss function with respect to the weights of the network\n","$w_{ij}^{(l)}$ is the weight for the connection between the $i^{th}$ neuron in the $l^{th}$ layer and the $j^{th}$ neuron in the $(l+1)^{th}$ layer\n","$z_{j}^{(l+1)}$ is the input to the activation function of the $j^{th}$ neuron in the $(l+1)^{th}$ layer\n","formula: $\\frac{\\partial L}{\\partial w_{ij}^{(l)}} = \\frac{\\partial L}{\\partial z_{j}^{(l+1)}} \\frac{\\partial z_{j}^{(l+1)}}{\\partial w_{ij}^{(l)}}$\n","```python"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Adaptive Learning Rate:\n","gradient descent algorithms:\n","- SGD: Stochastic Gradient Descent\n","```python\n","tf.keras.optimizers.SGD\n","```\n","- Adam: adaptive moment estimation\n","```python\n","tf.keras.optimizers.Adam\n","```\n","- AdaGrad: adaptive gradient algorithm\n","```python\n","tf.keras.optimizers.Adagrad\n","```"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Putting it together:\n","```python\n","model = tf.keras.Sequential([...])\n","optimizer = tf.keras.optimizers.SGD()\n","while True:\n","    prediction = model(x) # Forward pass\n","    with tf.GradientTape() as tape:\n","        loss = compute_loss(y, prediction) # Compute loss\n","    grads = tape.gradient(loss, model.trainable_variables) # Compute gradient\n","    optimizer.apply_gradients(zip(grads, model.trainable_variables)) # Update weights\n","\n","```"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Stochastic Gradient Descent:\n","process:\n","- initialize weights with random values\n","- loop until convergence:\n","    - select a random subset of data points\n","    - calculate the gradient of the loss with respect to the weights\n","    - update the weights\n","- return the weights"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Regularization:\n","## Dropout:\n","- randomly set some activations to zero\n","- The idea is that by dropping some neurons from the network, we are removing the co-dependency between neurons. This forces the network to learn redundant representations, which makes it more robust and less likely to overfit the training data.\n","```python\n","tf.keras.layers.Dropout(rate)\n","# dropout rate: the fraction of the input units to drop in a layer\n","```\n","\n","## Early Stopping:\n","- on overfitting model, the validation loss will start to increase after a certain point, while the training loss will continue to decrease\n","- stop training when the validation loss starts to increase\n","```python\n","tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=0)\n","```\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
